{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 5\n",
    "var 5\n",
    "Model training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Определяет, будет ли использоваться устройство \"cuda\" (если доступно) или \"cpu\" \n",
    "Устанавливает seed для генерации случайных чисел и выводит выбранное устройство.\n",
    "'''\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1234)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделение набора данных на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Создает кастомный датасет для обработки изображений. Класс `CustomDataset` наследует от `torch.utils.data.Dataset` и определяет методы `__init__`, `__len__` и `__getitem__`.\n",
    "Он загружает изображения из файлового списка, применяет преобразования (если они указаны) и возвращает преобразованные изображения с соответствующими метками.\n",
    "\n",
    "Создаются экземпляры кастомного датасета для обучающего, тестового и валидационного наборов данных, используя соответствующие списки файлов и преобразования.\n",
    "\n",
    "Выводит информацию о втором элементе из обучающего датасета и метке первого элемента из валидационного датасета.\n",
    "'''\n",
    "val_path = os.path.abspath('val_list')\n",
    "test_path = os.path.abspath('test_list')\n",
    "train_path = os.path.abspath('train_list')\n",
    "\n",
    "df = pd.read_csv('annotations_3.csv', sep=',', header=None)\n",
    "df = df.drop(df.index[0])\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "df.rename(columns={1: 'Absolute_path', 2: 'Class'}, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "print(df)\n",
    "\n",
    "for path in [val_path, test_path, train_path]:\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "def load_image(df, path, i):\n",
    "    image_path = os.path.abspath(os.path.join(*df.Absolute_path[i].split(\"\\\\\")))\n",
    "    image = cv2.imread(image_path)\n",
    "    cv2.imwrite(os.path.join(path, f'{i}.jpg'), image)\n",
    "\n",
    "for i in range(900, 1000):\n",
    "    load_image(df, val_path, i)\n",
    "for i in range(1900, 1999):\n",
    "    load_image(df, val_path, i)\n",
    "for i in range(800, 900):\n",
    "    load_image(df, test_path, i)\n",
    "for i in range(1800, 1900):\n",
    "    load_image(df, test_path, i)\n",
    "for i in range(800):\n",
    "    load_image(df, train_path, i)\n",
    "\n",
    "train_list = glob.glob(os.path.join(train_path, '*.jpg'))\n",
    "test_list = glob.glob(os.path.join(test_path, '*.jpg'))\n",
    "\n",
    "train_list, val_list = train_test_split(train_list, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Генерирует 10 случайных индексов из диапазона от 0 до 199 без повторений\n",
    "Отображает изображения из обучающего списка с использованием библиотеки matplotlib.\n",
    "'''\n",
    "random_idx = np.random.choice(200, size=10, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = cv2.imread(train_list[random_idx[i]])\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увелечение изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Определяет наборы преобразований для обучающего, валидационного и тестового наборов данных\n",
    "таких как изменение размера, случайное обрезание, горизонтальное отражение и преобразование в тензоры.\n",
    "'''\n",
    "common_transforms = [\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "    ]\n",
    "\n",
    "train_transforms = transforms.Compose(common_transforms)\n",
    "val_transforms = transforms.Compose(common_transforms)\n",
    "test_transforms = transforms.Compose(common_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Определяет пути к директориям для валидационного, тестового и обучающего наборов данных.\n",
    "Считывает и предобрабатывает CSV-файл с аннотациями, затем создает директории, если они не существуют.\n",
    "Определяет функцию для загрузки и сохранения изображений.\n",
    "Обрабатывает изображения на основе заданных диапазонов.\n",
    "Создает списки файлов изображений и разделяет обучающий список на обучающий и валидационный наборы данных.\n",
    "'''\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img_transformed = self.transform(img)\n",
    "        else:\n",
    "            img_transformed = img\n",
    "\n",
    "        label = 1 if 'brown bear' in img_path else 0 \n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Создание кастомного датасета для обучения, тестирования и валидации модели\n",
    "C использованием предварительно определенных преобразований.\n",
    "'''\n",
    "train_data = CustomDataset(train_list, transform=train_transforms)\n",
    "test_data = CustomDataset(test_list, transform=test_transforms)\n",
    "val_data = CustomDataset(val_list, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[1])\n",
    "print(val_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders():\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=batch_size, shuffle=True)\n",
    "    return (train_loader, test_loader, val_loader)\n",
    "\n",
    "train_loader, test_loader, val_loader = create_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(val_data), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv,self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,16,kernel_size=3, padding=0,stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,32, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3*3*64,10),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(train_loader))\n",
    "print(len(val_data), len(val_loader))\n",
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 5-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True )\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train(model, epochs):\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  epoch_val_acc_list = []\n",
    "  epoch_val_loss_list = []\n",
    "  epoch_acc_list = []\n",
    "  epoch_loss_list = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      epoch_loss = 0\n",
    "      epoch_acc = 0\n",
    "\n",
    "      for data, label in train_loader:\n",
    "          data = data.to(device)\n",
    "          label = label.to(device)\n",
    "\n",
    "          output = model(data)\n",
    "          loss = criterion(output, label)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          acc = ((output.argmax(dim=1) == label).float().mean())\n",
    "          epoch_acc += acc/len(train_loader)\n",
    "          epoch_loss += loss/len(train_loader)\n",
    "\n",
    "      print('Epoch : {}, train acc : {}, train loss : {}'.format(epoch+1, epoch_acc,epoch_loss))\n",
    "      epoch_acc_list.append(epoch_acc)\n",
    "      epoch_loss_list.append(epoch_loss)\n",
    "      with torch.no_grad():\n",
    "          epoch_val_acc=0\n",
    "          epoch_val_loss =0\n",
    "          for data, label in val_loader:\n",
    "              data = data.to(device)\n",
    "              label = label.to(device)\n",
    "              val_output = model(data)\n",
    "              val_loss = criterion(val_output,label)\n",
    "\n",
    "              acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
    "              epoch_val_acc += acc/ len(val_loader)\n",
    "              epoch_val_loss += val_loss/ len(val_loader)\n",
    "\n",
    "          print('Epoch : {}, val_acc : {}, val_loss : {}'.format(epoch+1, epoch_val_acc,epoch_val_loss))\n",
    "\n",
    "      epoch_val_acc_list.append(epoch_val_acc)\n",
    "      epoch_val_loss_list.append(epoch_val_loss)\n",
    "\n",
    "  return(epoch_val_acc_list,epoch_val_loss_list,\n",
    "  epoch_acc_list,\n",
    "  epoch_loss_list)\n",
    "\n",
    "epoch_val_acc_list,epoch_val_loss_list,epoch_acc_list,epoch_loss_list = train(model, epochs)\n",
    "\n",
    "def show_plot(lis, list_val, text):\n",
    "  x = list(range(1, len(lis)+ 1))\n",
    "  plt.plot(x, lis, label = text+\" train\")\n",
    "  plt.plot(x, list_val, label = text+\" test\")\n",
    "  plt.title(text)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "show_plot([i.data for i in epoch_loss_list], epoch_val_loss_list, \"loss\")\n",
    "show_plot(epoch_acc_list, epoch_val_acc_list, \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005 # lr\n",
    "batch_size = 10 # we will use mini-batch method\n",
    "\n",
    "train_loader, test_loader, val_loader = create_loaders()\n",
    "\n",
    "epoch_val_acc_list,epoch_val_loss_list,epoch_acc_list,epoch_loss_list = train(model, epochs)\n",
    "\n",
    "show_plot([i.data for i in epoch_loss_list], epoch_val_loss_list, \"loss\")\n",
    "show_plot(epoch_acc_list, epoch_val_acc_list, \"acc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
